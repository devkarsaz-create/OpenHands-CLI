# TUI End-to-End Tests for OpenHands CLI

This directory contains end-to-end tests that run the actual OpenHands CLI TUI binary built by PyInstaller.

## Structure

- `models.py` - Pydantic models for test results (`TestResult` and `TestSummary`)
- `runner.py` - Test runner that coordinates all tests and provides summary reporting
- `test_version.py` - Tests the `--version` flag functionality
- `test_experimental_ui.py` - Tests the textual UI functionality
- `test_acp.py` - Tests the ACP server functionality with JSON-RPC messages
- `mock_llm_server.py` - Mock LLM server with trajectory replay for deterministic e2e testing
- `trajectory.py` - Trajectory loading and management utilities

## Trajectory-Based Testing

E2E tests use **trajectory replay** to ensure deterministic, reproducible testing. A trajectory is a recorded sequence of events from a real agent conversation that can be replayed by the mock LLM server.

### Trajectory Structure

Trajectories are stored in `tests/trajectories/<name>/` as individual JSON event files:

```
tests/trajectories/simple_echo_hello_world/
├── event-00001-<uuid>.json  # MessageEvent (user input)
├── event-00002-<uuid>.json  # ActionEvent (agent tool call)
├── event-00003-<uuid>.json  # ObservationEvent (tool result)
└── event-00004-<uuid>.json  # MessageEvent (agent response)
```

### Event Types

Each event file contains a JSON object with a `kind` field:

- **MessageEvent** (`source: "user"`): User input - sent by e2e test as simulated input
- **ActionEvent** (`source: "agent"`): Agent tool calls - replayed by mock LLM server
- **ObservationEvent** (`source: "environment"`): Tool results - generated by actual execution
- **MessageEvent** (`source: "agent"`): Agent text responses - replayed by mock LLM server

### How It Works

1. **E2E test loads trajectory**: The test loads a trajectory and extracts user inputs
2. **Mock server loads LLM responses**: The server loads ActionEvents and agent MessageEvents
3. **Test sends user input**: The e2e test sends each user message to the CLI
4. **Mock server replays responses**: Each `/chat/completions` request returns the next LLM response in sequence
5. **CLI executes tool calls**: The actual CLI executes tool calls (e.g., terminal commands)
6. **Validation**: Test validates the output matches expected behavior

## Mock LLM Server

The `mock_llm_server.py` module provides a mock OpenAI-compatible LLM server that replays responses from trajectory files.

### Features

- OpenAI-compatible `/chat/completions` endpoint
- Replays `tool_calls` from ActionEvents
- Replays text content from agent MessageEvents
- Streaming and non-streaming support
- Health check with remaining response count
- Reset endpoint to replay trajectory again

### Usage Example

```python
from tui_e2e.trajectory import load_trajectory
from tui_e2e.mock_llm_server import MockLLMServer

# Load trajectory
trajectory = load_trajectory("tests/trajectories/simple_echo_hello_world")

# Start server with trajectory
server = MockLLMServer(trajectory=trajectory)
base_url = server.start()

# Get user inputs to send
user_inputs = trajectory.get_user_inputs()
for event in user_inputs:
    user_text = event.get_user_text()
    # Send user_text to CLI...

# Configure environment for CLI
env = {
    "LLM_API_KEY": "mock-api-key",
    "LLM_BASE_URL": base_url,
    "LLM_MODEL": "openai/gpt-4o-mock",
}

# Run test...

# Stop server
server.stop()
```

### Running Mock Server Standalone

```bash
# Without trajectory (returns default responses)
python -m tui_e2e.mock_llm_server

# With trajectory
python -m tui_e2e.mock_llm_server tests/trajectories/simple_echo_hello_world
```

## Usage

The tests are automatically run by `build.py` after building the executable. Each test returns a `TestResult` object with:

- `test_name`: Name of the test
- `success`: Whether the test passed
- `cost`: Cost of running the test (currently always 0.0)
- `boot_time_seconds`: Time to boot the application (if applicable)
- `total_time_seconds`: Total test execution time
- `error_message`: Error message if the test failed
- `output_preview`: Preview of output for debugging
- `metadata`: Additional test-specific metadata

## Running Tests Manually

```python
from tui_e2e.runner import run_all_tui_e2e, print_detailed_results

summary = run_all_tui_e2e()
print_detailed_results(summary)
```

## Adding New Tests

### 1. Record a Trajectory

Run a real conversation and save the events to `tests/trajectories/<name>/`:

```python
# Events are automatically saved during conversation
# Or manually create event JSON files following the schema
```

### 2. Create Test File

```python
# test_my_feature.py
from .trajectory import load_trajectory
from .mock_llm_server import MockLLMServer
from .models import TestResult

def test_my_feature() -> TestResult:
    trajectory = load_trajectory("tests/trajectories/my_trajectory")
    
    with MockLLMServer(trajectory=trajectory) as server:
        # Run CLI with mock LLM
        # Validate results
        pass
    
    return TestResult(
        test_name="my_feature",
        success=True,
        total_time_seconds=elapsed,
    )
```

### 3. Register in Runner

Add the test function to `runner.py`:

```python
from .test_my_feature import test_my_feature

tests = [
    test_version,
    test_experimental_ui,
    test_acp_executable,
    test_my_feature,  # Add here
]
```

## Available Trajectories

| Name | Description |
|------|-------------|
| `simple_echo_hello_world` | Basic test: user asks to echo hello world |